{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importando librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "from keras import layers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import save_img\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPool2D, LeakyReLU\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Se elije una carpeta desde donde se trabaja\n",
    "mainpathlocation = '/media/fermin/Data/CosasUniversidad/Material/ULA6-Semestre12/Taller-de-Titulo-II/Recursos/HAM10000/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)\n",
    "tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "#gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "#for device in gpu_devices:\n",
    "#    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creando labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se lee el csv y se crean las labels de las clases\n",
    "metadf = pd.read_csv(mainpathlocation+'HAM10000_metadata.csv')\n",
    "le = LabelEncoder()\n",
    "le.fit(metadf['dx'])\n",
    "LabelEncoder()\n",
    "print(list(le.classes_))\n",
    "\n",
    "metadf['label'] = le.transform(metadf[\"dx\"])\n",
    "##print(metadf.sample(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Actinic Keratoses (akiec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Basal Cell Carcinoma (bcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Benign Keratosis-like (bkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Dermatofibroma (df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Melanoma (mel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Vascular Lesions (vas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Melanocytic Nevi (nv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Añadir las imágenes al dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(mainpathlocation)\n",
    "image_path = {os.path.splitext(os.path.basename(x))[0]: x\n",
    "              for x in glob(os.path.join(mainpathlocation+'HAMTESTS/n6Resized/','*.jpg'))}\n",
    "metadf['path'] = metadf['image_id'].map(image_path.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(mainpathlocation)\n",
    "sizeX = 100\n",
    "sizeY = 75\n",
    "##Para normalizar y cambiar el tamaño con sizeX y sizeY\n",
    "resize_and_rescale = tf.keras.Sequential([\n",
    "  layers.Resizing(sizeY, sizeX),\n",
    "  layers.Rescaling(1./255)\n",
    "])\n",
    "\n",
    "metadf['image'] = metadf['path'].map(lambda x: np.asarray(resize_and_rescale(Image.open(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtención de los datasets (por ahora solo el HAM10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.asarray(metadf['image'].tolist())\n",
    "Y = metadf['label']\n",
    "Y_cat = to_categorical(Y, num_classes=7)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y_cat, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(Conv2D(256, (3, 3), activation=\"relu\", input_shape=(sizeY, sizeX, 3)))\n",
    "model.add(Conv2D(256, (3, 3)))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), padding='same'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3)))\n",
    "model.add(Conv2D(128, (3, 3)))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "epochs = 50\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test,y_test), verbose=2)\n",
    "model.save(\"navidadCompleja.keras\")\n",
    "np.save('navidadComplejaHistory.npy', history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_model = tf.keras.models.load_model(\"navidad.h5\")\n",
    "#history=np.load('navidadComplejaHistory.npy',allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluación de precisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eval = model.evaluate(x_test, y_test, verbose=1)\n",
    "#test_eval = loaded_model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Acc:', test_eval[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicción de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicciones = model.predict(x_test)\n",
    "#predicciones = loaded_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = random.randrange(len(x_test))\n",
    "print(\"La imágen ingresada es:\")\n",
    "_ = plt.imshow(x_test[seed])\n",
    "print(np.argmax(y_test[seed]))\n",
    "predicciones_classes = []\n",
    "for prediction in predicciones:\n",
    "    predicciones_classes.append(prediction.tolist().index(max(prediction)))\n",
    "predicciones_classes = np.array(predicciones_classes)\n",
    "print(\"La predicción es:\")\n",
    "print(predicciones_classes[seed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('Epochs')\n",
    "plt.ylabel(\"Magnitud de Pérdida\")\n",
    "plt.plot(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = history.history['acc']\n",
    "val_accuracy = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(len(accuracy))\n",
    "plt.plot(epochs, accuracy, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
